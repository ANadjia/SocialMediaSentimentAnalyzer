{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be9bb142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ca553",
   "metadata": {},
   "source": [
    "# Read Data_1 and Data_2 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2bb2de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('../Data/data_1/PreProcessedYouTubeDataFileAndAnnotated.csv')\n",
    "data_1[\"text\"] = data_1[\"text\"].apply(lambda x : str(x).split())\n",
    "data_1 = data_1['text'].tolist()\n",
    "\n",
    "\n",
    "data_2 = pd.read_csv('../Data/data_2/PreProcessedYouTubeDataFileAndAnnotated.csv')\n",
    "data_2[\"text\"] = data_2[\"text\"].apply(lambda x : str(x).split())\n",
    "data_2 = data_2['text'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d6c7ad",
   "metadata": {},
   "source": [
    "##  Read training and testing data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16085b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our case , the size of data in : [15,25,35,50]\n",
    "def read_data(num_data, test_size):  \n",
    "    X_train = pd.read_csv(f'../Data/data_{num_data}/Training_data/X_{test_size}.csv')['text'].apply(lambda x : str(x).split()).to_list()\n",
    "    X_test = pd.read_csv(f'../Data/data_{num_data}/Testing_data/X_{test_size}.csv')['text'].apply(lambda x : str(x).split()).to_list()\n",
    "    y_train = pd.read_csv(f'../Data/data_{num_data}/Training_data/Y_{test_size}.csv')['offensive/non offensive'].to_list()\n",
    "    y_test = pd.read_csv(f'../Data/data_{num_data}/Testing_data/Y_{test_size}.csv')['offensive/non offensive'].to_list()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1bb680bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = pd.read_csv('../Data/data_2/Training_data/X.csv')['text'].apply(lambda x : str(x).split()).to_list()\n",
    "X_test_2 = pd.read_csv('../Data/data_2/Testing_data/X.csv')['text'].apply(lambda x : str(x).split()).to_list()\n",
    "y_train_2 = pd.read_csv('../Data/data_2/Training_data/Y.csv')['offensive/non offensive'].to_list()\n",
    "y_test_2 = pd.read_csv('../Data/data_2/Testing_data/Y.csv')['offensive/non offensive'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0155fde",
   "metadata": {},
   "source": [
    "# Create Embedding : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944aa810",
   "metadata": {},
   "source": [
    " Set values for various parameters : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c3df55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 300    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 2   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "epoch = 50 # Number of epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e238ce",
   "metadata": {},
   "source": [
    "##  FastText :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46888ac2",
   "metadata": {},
   "source": [
    "####      For  Data 1 :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "03ed594d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'جايح': ['جايحة', 'وجايحة', 'طايح', 'جايحين', 'يسخف'], 'خماج': ['لخماج', 'الخماج', 'يتزوج', 'عجوزة', 'خدمتها'], 'رخس': ['مرخس', 'خانزة', 'شبيبة', 'يزيدو', 'بقاو']}\n",
      "{'جايح': ['جايحة', 'جاي', 'خمي', 'وجايحة', 'طايح'], 'خماج': ['لخماج', 'تخدمي', 'جبهة', 'تكذبي', 'نكرهك'], 'رخس': ['رخيسا', 'مرخس', 'كشغل', 'تفو', 'مطار']}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# sg decides whether to use the skip-gram model (1) or CBOW (0)\n",
    "ft_model_sg_1 = FastText(sentences=data_1, window=window_context, vector_size=vec_size, \n",
    "                    min_count=min_word_count,sample=sample, sg=1, epochs=epoch)\n",
    "                                 \n",
    "\n",
    "ft_model_cbow_1 = FastText(sentences=data_1, window=window_context, vector_size=vec_size, \n",
    "                    min_count=min_word_count,sample=sample, sg=0, epochs=epoch)\n",
    "\n",
    "\n",
    "# view similar words based on gensim's FastText model\n",
    "similar_words_sg_1 = {search_term: [item[0] for item in ft_model_sg_1.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['جايح', 'خماج','رخس']}\n",
    "print(similar_words_sg_1) \n",
    "\n",
    "similar_words_cbow_1 = {search_term: [item[0] for item in ft_model_cbow_1.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['جايح', 'خماج','رخس']}\n",
    "print(similar_words_cbow_1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1176e",
   "metadata": {},
   "source": [
    "Save the mpodels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3649de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ft_model_sg_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8120d9818b99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mft_model_sg_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../EmbeddingModels/ft_model_sg_1.model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mft_model_cbow_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../EmbeddingModels/ft_model_cbow_1.model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ft_model_sg_1' is not defined"
     ]
    }
   ],
   "source": [
    "ft_model_sg_1.save(\"../EmbeddingModels/ft_model_sg_1.model\")\n",
    "ft_model_cbow_1.save(\"../EmbeddingModels/ft_model_cbow_1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b16cdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17157573\n",
      "0.19886199\n",
      "0.1983188\n",
      "0.2696668\n"
     ]
    }
   ],
   "source": [
    "print(ft_model_sg_1.wv.similarity(w1='سكر', w2='حمار'))\n",
    "print(ft_model_sg_1.wv.similarity(w1='خماج', w2='حمار'))\n",
    "print(ft_model_sg_1.wv.similarity(w1='خماج', w2='رخس'))\n",
    "print(ft_model_sg_1.wv.similarity(w1='سكر', w2='رخس'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efaddc6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48251474\n",
      "0.50533843\n",
      "0.4563963\n",
      "0.3147485\n"
     ]
    }
   ],
   "source": [
    "print(ft_model_cbow_1.wv.similarity(w1='سكر', w2='حمار'))\n",
    "print(ft_model_cbow_1.wv.similarity(w1='خماج', w2='حمار'))\n",
    "print(ft_model_cbow_1.wv.similarity(w1='خماج', w2='رخس'))\n",
    "print(ft_model_cbow_1.wv.similarity(w1='سكر', w2='رخس'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c653e9b",
   "metadata": {},
   "source": [
    "####      For  Data 2 :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4e403c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'جايح': ['جايحة', 'وجايحة', 'جايحين', 'فايح', 'جاية'], 'خماج': ['لخماج', 'يالخماج', 'الخماج', 'مونطاج', 'نتوقعو'], 'رخس': ['مرخس', 'طيحلهم', 'الرخس', 'خانزة', 'شبعتي']}\n",
      "{'جايح': ['جايحة', 'وجايحة', 'جاي', 'توريلي', 'تخدمي'], 'خماج': ['دوماج', 'لخماج', 'يالخماج', 'مساج', 'ديڨاج'], 'رخس': ['رخيسا', 'يندبو', 'مرخس', 'رخيص', 'رخيصة']}\n"
     ]
    }
   ],
   "source": [
    "# sg decides whether to use the skip-gram model (1) or CBOW (0)\n",
    "ft_model_sg_2 = FastText(sentences=data_2, window=window_context, vector_size=vec_size, \n",
    "                    min_count=min_word_count,sample=sample, sg=1, epochs=epoch)\n",
    "                                 \n",
    "\n",
    "ft_model_cbow_2 = FastText(sentences=data_2, window=window_context, vector_size=vec_size, \n",
    "                    min_count=min_word_count,sample=sample, sg=0, epochs=epoch)\n",
    "\n",
    "\n",
    "# view similar words based on gensim's FastText model\n",
    "similar_words_sg_2 = {search_term: [item[0] for item in ft_model_sg_2.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['جايح', 'خماج','رخس']}\n",
    "print(similar_words_sg_2) \n",
    "\n",
    "similar_words_cbow_2 = {search_term: [item[0] for item in ft_model_cbow_2.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['جايح', 'خماج','رخس']}\n",
    "print(similar_words_cbow_2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1a00d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_sg_2.save(\"../EmbeddingModels/ft_model_sg_2.model\")\n",
    "ft_model_cbow_2.save(\"../EmbeddingModels/ft_model_cbow_2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "208d844f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3457801\n",
      "0.22169714\n",
      "0.19174257\n",
      "0.31059802\n"
     ]
    }
   ],
   "source": [
    "print(ft_model_sg_2.wv.similarity(w1='سكر', w2='حمار'))\n",
    "print(ft_model_sg_2.wv.similarity(w1='خماج', w2='حمار'))\n",
    "print(ft_model_sg_2.wv.similarity(w1='خماج', w2='رخس'))\n",
    "print(ft_model_sg_2.wv.similarity(w1='سكر', w2='رخس'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0cb3882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5365528\n",
      "0.37388217\n",
      "0.5259753\n",
      "0.3429577\n"
     ]
    }
   ],
   "source": [
    "print(ft_model_cbow_2.wv.similarity(w1='سكر', w2='حمار'))\n",
    "print(ft_model_cbow_2.wv.similarity(w1='خماج', w2='حمار'))\n",
    "print(ft_model_cbow_2.wv.similarity(w1='خماج', w2='رخس'))\n",
    "print(ft_model_cbow_2.wv.similarity(w1='سكر', w2='رخس'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa551746",
   "metadata": {},
   "source": [
    "##  Word2vec : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ff65e",
   "metadata": {},
   "source": [
    "### For Data 1 :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa395703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'خماج': ['فايحا', 'يتزوج', 'ياعباد', 'عجوزة', 'قدها'], 'رخس': ['خانزة', 'شبيبة', 'يحبو', 'والديها', 'وراهي']}\n",
      "{'خماج': ['قدها', 'يتزوج', 'صباط', 'شعرها', 'مكتوب'], 'رخس': ['فحلات', 'كرهت', 'ليجان', 'بقيمة', 'غلطتي']}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# sg decides whether to use the skip-gram model (1) or CBOW (0)\n",
    "w2v_model_sg_1 = Word2Vec(sentences=data_1, window=window_context, vector_size=vec_size, \n",
    "                    min_count=min_word_count,sample=sample, sg=1, epochs=epoch)\n",
    "\n",
    "\n",
    "w2v_model_cbow_1 = Word2Vec(sentences=data_1, window=window_context, vector_size=vec_size, \n",
    "                    min_count=min_word_count,sample=sample, sg=0, epochs=epoch)\n",
    "\n",
    "\n",
    "# view similar words based on gensim's Word2vec model\n",
    "similar_words_sg_1 = {search_term: [item[0] for item in w2v_model_sg_1.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in [ 'خماج','رخس']}\n",
    "print(similar_words_sg_1) \n",
    "\n",
    "similar_words_cbow_1 = {search_term: [item[0] for item in w2v_model_cbow_1.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in [ 'خماج','رخس']}\n",
    "print(similar_words_cbow_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f56058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_sg_1.save(\"../EmbeddingModels/w2v_model_sg_1.model\")\n",
    "w2v_model_cbow_1.save(\"../EmbeddingModels/w2v_model_cbow_1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7620885a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('خانزة', 0.6619195342063904),\n",
       " ('شبيبة', 0.6177570223808289),\n",
       " ('يحبو', 0.5956509709358215),\n",
       " ('والديها', 0.5819247364997864),\n",
       " ('وراهي', 0.5684953927993774),\n",
       " ('بعدها', 0.5655162930488586),\n",
       " ('عقول', 0.5439682602882385),\n",
       " ('وممبعد', 0.5403951406478882),\n",
       " ('يزيدو', 0.5390001535415649),\n",
       " ('يجو', 0.5312548875808716)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_sg_1.wv.most_similar(positive=\"رخس\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82a0e6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('فحلات', 0.8429617285728455),\n",
       " ('كرهت', 0.8335102200508118),\n",
       " ('ليجان', 0.8243767023086548),\n",
       " ('بقيمة', 0.822964608669281),\n",
       " ('غلطتي', 0.811455488204956),\n",
       " ('بردتلي', 0.8099099397659302),\n",
       " ('سمحلي', 0.8087772130966187),\n",
       " ('جزاير', 0.8086364269256592),\n",
       " ('يكحل', 0.8056181073188782),\n",
       " ('يحبو', 0.8044017553329468)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_cbow_1.wv.most_similar(positive=\"رخس\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc05415",
   "metadata": {},
   "source": [
    "### For Data 2 :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "048fe93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'خماج': ['خاه', 'اختفوه', 'هبلت', 'هع', 'حابت'], 'رخس': ['للجزاءر', 'بالمليارات', 'خانزة', 'طيحلهم', 'السياسة']}\n",
      "{'خماج': ['هبلت', 'كشفها', 'جابلي', 'كيشغل', 'خطك'], 'رخس': ['حمر', 'قادرين', 'الصحراء', 'رجولة', 'وخيرمن']}\n"
     ]
    }
   ],
   "source": [
    "# sg decides whether to use the skip-gram model (1) or CBOW (0)\n",
    "w2v_model_sg_2 = Word2Vec(sentences=data_2, window=window_context, vector_size=vec_size, \n",
    "                    min_count=min_word_count,sample=sample, sg=1, epochs=epoch)\n",
    "\n",
    "\n",
    "w2v_model_cbow_2 = Word2Vec(sentences=data_2, window=window_context, vector_size=vec_size, \n",
    "                    min_count=min_word_count,sample=sample, sg=0, epochs=epoch)\n",
    "\n",
    "\n",
    "# view similar words based on gensim's Word2vec model\n",
    "similar_words_sg_2 = {search_term: [item[0] for item in w2v_model_sg_2.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in [ 'خماج','رخس']}\n",
    "print(similar_words_sg_2) \n",
    "\n",
    "similar_words_cbow_2 = {search_term: [item[0] for item in w2v_model_cbow_2.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in [ 'خماج','رخس']}\n",
    "print(similar_words_cbow_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c00e0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_sg_2.save(\"../EmbeddingModels/w2v_model_sg_2.model\")\n",
    "w2v_model_cbow_2.save(\"../EmbeddingModels/w2v_model_cbow_2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af6aefa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('للجزاءر', 0.5684206485748291),\n",
       " ('بالمليارات', 0.5521996021270752),\n",
       " ('خانزة', 0.5207986235618591),\n",
       " ('طيحلهم', 0.5147939324378967),\n",
       " ('السياسة', 0.4938235878944397),\n",
       " ('صحاح', 0.48775988817214966),\n",
       " ('شبعتي', 0.47357961535453796),\n",
       " ('يرضى', 0.47314271330833435),\n",
       " ('الانثى', 0.4718368947505951),\n",
       " ('عيناني', 0.46605417132377625)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_sg_2.wv.most_similar(positive=\"رخس\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afff42d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('حمر', 0.7169661521911621),\n",
       " ('قادرين', 0.715872585773468),\n",
       " ('الصحراء', 0.6898380517959595),\n",
       " ('رجولة', 0.6856645345687866),\n",
       " ('وخيرمن', 0.675184965133667),\n",
       " ('القنواة', 0.6721047759056091),\n",
       " ('ترحيب', 0.6674655079841614),\n",
       " ('نضيف', 0.6667680144309998),\n",
       " ('عشات', 0.6594468951225281),\n",
       " ('وفو', 0.6565319299697876)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_cbow_2.wv.most_similar(positive=\"رخس\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afe9b4",
   "metadata": {},
   "source": [
    "# Get word vector function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5dd69cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size, model):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "            \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde0f80",
   "metadata": {},
   "source": [
    "#  Embedding Matrix :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e6215aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(data, vec_size, model):\n",
    "    arrays = np.zeros((len(data), vec_size))\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        arrays[i,:] = word_vector(data[i], vec_size , model)\n",
    "\n",
    "    result = pd.DataFrame(arrays)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d87007",
   "metadata": {},
   "source": [
    "### FastText and Skip-Gram :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cf379b",
   "metadata": {},
   "source": [
    "Embedding Matrix for training data from the Data_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "83ffb755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2181, 300)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1_ft_sg = get_matrix(X_train_1,vec_size,ft_model_sg_1)\n",
    "X_train_1_ft_sg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0efc5",
   "metadata": {},
   "source": [
    "Embedding Matrix for testing data from the Data_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bdd44c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-bc7d37fbb7ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_test_1_ft_sg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvec_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mft_model_sg_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_test_1_ft_sg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test_1' is not defined"
     ]
    }
   ],
   "source": [
    "X_test_1_ft_sg = get_matrix(X_test_1,vec_size,ft_model_sg_1)\n",
    "X_test_1_ft_sg.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709b7cc",
   "metadata": {},
   "source": [
    "Embedding Matrix for training data from the Data_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e38fa06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5157, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2_ft_sg = get_matrix(X_train_2,vec_size,ft_model_sg_2)\n",
    "X_train_2_ft_sg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11092e7a",
   "metadata": {},
   "source": [
    "Embedding Matrix for testing data from the Data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bb3f0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(911, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_2_ft_sg = get_matrix(X_test_2,vec_size,ft_model_sg_2)\n",
    "X_test_2_ft_sg.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7e50f",
   "metadata": {},
   "source": [
    "### FastText and CBOW :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1c57a",
   "metadata": {},
   "source": [
    "Embedding Matrix for training data from the Data_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40181a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2181, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1_ft_cbow = get_matrix(X_train_1,vec_size,ft_model_cbow_1)\n",
    "X_train_1_ft_cbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4774a2d0",
   "metadata": {},
   "source": [
    "Embedding Matrix for testing data from the Data_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eeb92c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_1_ft_cbow = get_matrix(X_test_1,vec_size,ft_model_cbow_1)\n",
    "X_test_1_ft_cbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5f3b3",
   "metadata": {},
   "source": [
    "Embedding Matrix for training data from the Data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91b7b805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5157, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2_ft_cbow = get_matrix(X_train_2,vec_size,ft_model_cbow_2)\n",
    "X_train_2_ft_cbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b39b6a",
   "metadata": {},
   "source": [
    "Embedding Matrix for testing data from the Data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "989ebf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(911, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_2_ft_cbow = get_matrix(X_test_2,vec_size,ft_model_cbow_2)\n",
    "X_test_2_ft_cbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9c35e",
   "metadata": {},
   "source": [
    "### Word2vec and Skip-Gram :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c7210",
   "metadata": {},
   "source": [
    "Embedding Matrix for training data from the Data_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ebf593b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2181, 300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1_w2v_sg = get_matrix(X_train_1,vec_size,w2v_model_sg_1)\n",
    "X_train_1_w2v_sg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6729e93",
   "metadata": {},
   "source": [
    "Embedding Matrix for testing data from the Data_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c7f5657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_1_w2v_sg = get_matrix(X_test_1,vec_size,w2v_model_sg_1)\n",
    "X_test_1_w2v_sg.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bcbf3",
   "metadata": {},
   "source": [
    "Embedding Matrix for training data from the Data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a73e144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5157, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2_w2v_sg = get_matrix(X_train_2,vec_size,w2v_model_sg_2)\n",
    "X_train_2_w2v_sg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af186607",
   "metadata": {},
   "source": [
    "Embedding Matrix for testing data from the Data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7d18843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(911, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_2_w2v_sg = get_matrix(X_test_2,vec_size,w2v_model_sg_2)\n",
    "X_test_2_w2v_sg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa25b9",
   "metadata": {},
   "source": [
    "### Word2vec and CBOW :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c78ea7",
   "metadata": {},
   "source": [
    "Embedding Matrix for training data from the Data_1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c248d277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2181, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1_w2v_cbow = get_matrix(X_train_1,vec_size,w2v_model_cbow_1)\n",
    "X_train_1_w2v_cbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7746d19f",
   "metadata": {},
   "source": [
    "Embedding Matrix for testing data from the Data_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5e52176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 300)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_1_w2v_cbow = get_matrix(X_test_1,vec_size,w2v_model_cbow_1)\n",
    "X_test_1_w2v_cbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd952bd",
   "metadata": {},
   "source": [
    "Embedding Matrix for training data from the Data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff848640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5157, 300)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2_w2v_cbow = get_matrix(X_train_2,vec_size,w2v_model_cbow_2)\n",
    "X_train_2_w2v_cbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51042506",
   "metadata": {},
   "source": [
    "Embedding Matrix for testing data from the Data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aab0b8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(911, 300)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_2_w2v_cbow = get_matrix(X_test_2,vec_size,w2v_model_cbow_2)\n",
    "X_test_2_w2v_cbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b458eb",
   "metadata": {},
   "source": [
    "# Training the models : --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b67963",
   "metadata": {},
   "source": [
    "## SVM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43dbf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    from sklearn import svm\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # classify using support vector classifier\n",
    "    svm = svm.SVC(kernel = 'linear', probability=True)\n",
    "\n",
    "    # fit the SVC model based on the given training data\n",
    "    prob = svm.fit(x_train, y_train).predict_proba(x_test)\n",
    "\n",
    "    # perform classification and prediction on samples in x_test\n",
    "    y_pred_svm = svm.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred_svm) * 100 \n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "605a367f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_1_w2v_cbow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-a2ebdae5926a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_1_w2v_cbow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_1_w2v_cbow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_1_w2v_cbow' is not defined"
     ]
    }
   ],
   "source": [
    "svm(X_train_1_w2v_cbow,X_test_1_w2v_cbow,y_train_1,y_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3df1d",
   "metadata": {},
   "source": [
    "#### Data 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a712f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_1_ft_sg = svm(X_train_1_ft_sg,X_test_1_ft_sg,y_train_1,y_test_1)\n",
    "accuracy_1_ft_cbow = svm(X_train_1_ft_cbow,X_test_1_ft_cbow,y_train_1,y_test_1)\n",
    "\n",
    "\n",
    "accuracy_1_w2v_sg = svm(X_train_1_w2v_sg,X_test_1_w2v_sg,y_train_1,y_test_1)\n",
    "accuracy_1_w2v_cbow = svm(X_train_1_w2v_cbow,X_test_1_w2v_cbow,y_train_1,y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4eea3db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results :\n",
      "accuracy using FastText 'SG' : 72.53886010362694 %\n",
      "accuracy using FastText 'CBOW' : 72.53886010362694 %\n",
      "accuracy using Word2vec 'SG' : 68.39378238341969 %\n",
      "accuracy using Word2vec 'CBOW' : 68.9119170984456 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Results :\")\n",
    "print(f\"accuracy using FastText 'SG' : {accuracy_1_ft_sg} %\")\n",
    "print(f\"accuracy using FastText 'CBOW' : {accuracy_1_ft_cbow} %\")\n",
    "\n",
    "print(f\"accuracy using Word2vec 'SG' : {accuracy_1_w2v_sg} %\")\n",
    "print(f\"accuracy using Word2vec 'CBOW' : {accuracy_1_w2v_cbow} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a29fc",
   "metadata": {},
   "source": [
    "#### Data 2 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d9f8062",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_2_ft_sg = svm(X_train_2_ft_sg,X_test_2_ft_sg,y_train_2,y_test_2)\n",
    "accuracy_2_ft_cbow = svm(X_train_2_ft_cbow,X_test_2_ft_cbow,y_train_2,y_test_2)\n",
    "\n",
    "\n",
    "accuracy_2_w2v_sg = svm(X_train_2_w2v_sg,X_test_2_w2v_sg,y_train_2,y_test_2)\n",
    "accuracy_2_w2v_cbow = svm(X_train_2_w2v_cbow,X_test_2_w2v_cbow,y_train_2,y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7baaa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results :\n",
      "accuracy using FastText 'SG' : 71.13062568605928 %\n",
      "accuracy using FastText 'CBOW' : 68.1668496158068 %\n",
      "accuracy using Word2vec 'SG' : 70.25246981339188 %\n",
      "accuracy using Word2vec 'CBOW' : 69.0450054884742 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Results :\")\n",
    "print(f\"accuracy using FastText 'SG' : {accuracy_2_ft_sg} %\")\n",
    "print(f\"accuracy using FastText 'CBOW' : {accuracy_2_ft_cbow} %\")\n",
    "\n",
    "print(f\"accuracy using Word2vec 'SG' : {accuracy_2_w2v_sg} %\")\n",
    "print(f\"accuracy using Word2vec 'CBOW' : {accuracy_2_w2v_cbow} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4e1d94",
   "metadata": {},
   "source": [
    "##  CNN : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617c3eb",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(data_1)\n",
    " \n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(data_1)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    " \n",
    "# load all test reviews\n",
    "positive_docs = process_docs('txt_sentoken/pos', vocab, False)\n",
    "negative_docs = process_docs('txt_sentoken/neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    " \n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cabe957",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "21107622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  11769\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FastText' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-208-5acf9d87332f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mnb_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0membedding_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mft_model_sg_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0membedding_vector\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_vector\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# words not found in embedding index will be all-zeros.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FastText' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data_1)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "\n",
    "\n",
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 8 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = ft_model_sg_1.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b488a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(data_1)\n",
    "\n",
    "#Get the Vocabulary :\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5992c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence encode\n",
    "encoded_data_1 = tokenizer.texts_to_sequences(data_1)\n",
    "\n",
    "encoded_X_train_1 = tokenizer.texts_to_sequences(X_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f0d105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the Max length of sentences : \n",
    "def get_max_length(data):\n",
    "    max_length = 0\n",
    "    for index in range(len(data)) : \n",
    "        number_words = len(data[index])\n",
    "        if (number_words) > (max_length):\n",
    "            max_length = number_words\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e965140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_data_1 = get_max_length(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "94b1f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padding_data_1 = pad_sequences(encoded_data_1,maxlen = max_len_data_1)\n",
    "padding_X_train_1 = pad_sequences(encoded_X_train_1,maxlen = max_len_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b4a2e1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model_sg_1.wv['قمر'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9113ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = len(word_index)\n",
    "total_words = unique_words + 1 \n",
    "skipped_words = 0 \n",
    "embedding_dim = 300\n",
    "\n",
    "embedding_matrix = np.zeros((total_words, embedding_dim))\n",
    "\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = None\n",
    "    try:\n",
    "        embedding_vector = ft_model_sg_1.wv[word]\n",
    "    except :\n",
    "        skipped_words += 1\n",
    "        pass\n",
    "    if embedding_vector is not None :\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3c4b5920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11770, 300)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0dd44807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "\n",
    "embedding_layer = Embedding(total_words, embedding_dim,\n",
    "          weights=[embedding_matrix], input_length=max_len_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fb3af8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8f4dd427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(40,5,activation=\"relu\"))\n",
    "model.add(MaxPooling1D(2,2))\n",
    "model.add(Conv1D(20,2,activation=\"relu\"))\n",
    "model.add(MaxPooling1D(2,2))\n",
    "model.add(Dense(10,activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "de297c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c449bfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 420, 300)          3531000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 416, 40)           60040     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 208, 40)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 207, 20)           1620      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 103, 20)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 103, 10)           210       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1030)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1031      \n",
      "=================================================================\n",
      "Total params: 3,593,901\n",
      "Trainable params: 3,593,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "36122544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "69/69 [==============================] - 6s 80ms/step - loss: 0.6891 - accuracy: 0.5525\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 5s 79ms/step - loss: 0.6649 - accuracy: 0.6029\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 5s 79ms/step - loss: 0.5860 - accuracy: 0.6836\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 5s 78ms/step - loss: 0.3642 - accuracy: 0.8423\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 5s 78ms/step - loss: 0.2067 - accuracy: 0.9046\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 5s 79ms/step - loss: 0.1504 - accuracy: 0.9266\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 5s 78ms/step - loss: 0.1310 - accuracy: 0.9358\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 5s 79ms/step - loss: 0.1175 - accuracy: 0.9335\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 5s 78ms/step - loss: 0.1086 - accuracy: 0.9422\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 5s 79ms/step - loss: 0.1073 - accuracy: 0.9409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b8b642fa30>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padding_X_train_1, np.array(y_train_1), epochs = 10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3dfdb6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2181,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_train_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "68e0b0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ef0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
