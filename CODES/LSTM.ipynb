{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b750b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d290d42",
   "metadata": {},
   "source": [
    "# #READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dada4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"../DATA/cleaned_data1.csv\", names=[\"tweet\", \"class\"]).iloc[2:,:]\n",
    "dataframe[\"class\"] = np.where(dataframe[\"class\"] == \"Positive\",1,0)\n",
    "dataframe = dataframe.sample(frac = 1)\n",
    "x= dataframe['tweet'].apply(lambda x : str(x).split()).to_list()\n",
    "y = dataframe['class'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16e5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test= train_test_split(x,y,test_size=0.25,random_state=42 , stratify=y)  #same random state \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50634fb",
   "metadata": {},
   "source": [
    "# Load word embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9261ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etudiant\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "# FastText :\n",
    "ft_model_sg   = FastText.load(\"../CODE/EmbeddingModels/ft_model_sg.model\")\n",
    "ft_model_cbow  = FastText.load(\"../CODE/EmbeddingModels/ft_model_cbow.model\")\n",
    "\n",
    "# Word2vec :\n",
    "w2v_model_sg   = Word2Vec.load(\"../CODE/EmbeddingModels/w2v_model_sg.model\")\n",
    "w2v_model_cbow = Word2Vec.load(\"../CODE/EmbeddingModels/w2v_model_cbow.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9105b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "embedding_dim = 300\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc695e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the Max length of sentences : \n",
    "def get_max_length(data):\n",
    "    max_length = 0\n",
    "    for index in range(len(data)) : \n",
    "        number_words = len(data[index])\n",
    "        if (number_words) > (max_length):\n",
    "            max_length = number_words\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "127481b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len_data = 500\n"
     ]
    }
   ],
   "source": [
    "max_len_data = get_max_length(x)\n",
    "max_len_data = 500\n",
    "\n",
    "print(f\"max_len_data = {max_len_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a5763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eef47b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total words : 174770\n"
     ]
    }
   ],
   "source": [
    "total_words = len(tokenizer.word_index) +1\n",
    "print(f\" Total words : {total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "982ce06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(model,data,embedding_dim, max_nb_words, total_words,tokenizer):\n",
    "     \n",
    "    skipped_words = 0 \n",
    "    embedding_matrix = np.zeros((total_words, embedding_dim))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        # embedding_vector = None\n",
    "        try:\n",
    "            embedding_vector = model.wv[word]\n",
    "        except :\n",
    "            skipped_words += 1\n",
    "            pass\n",
    "        if embedding_vector is not None :\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "983b7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_ft_sg    = get_embedding_matrix(ft_model_sg,   x,embedding_dim,MAX_NB_WORDS,total_words,tokenizer)\n",
    "embedding_matrix_ft_cbow  = get_embedding_matrix(ft_model_cbow, x,embedding_dim,MAX_NB_WORDS,total_words,tokenizer)\n",
    "embedding_matrix_w2v_sg   = get_embedding_matrix(w2v_model_sg,  x,embedding_dim,MAX_NB_WORDS,total_words,tokenizer)\n",
    "embedding_matrix_w2v_cbow = get_embedding_matrix(w2v_model_cbow,x,embedding_dim,MAX_NB_WORDS,total_words,tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f721f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Data  :\n",
    "\n",
    "padding_X_train = pad_sequences(tokenizer.texts_to_sequences(X_train),maxlen = max_len_data)\n",
    "padding_X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen = max_len_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13f8f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM , Embedding , Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19abde32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtype=object\n",
    "#X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "178ffc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.reshape(X_train, newshape = range(X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60636136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.reshape(X_train,(len(X_train),max_len_data,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df7c37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(embedding_layer):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    \n",
    "    model.compile(optimizer= 'adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2dadb013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test,predictions, file ,info ):\n",
    "    file.write(\"\\n Le model : \" + str(info)+\"\\n\")  \n",
    "    file.write(\"Confusion Matrix : \\n\" + str(confusion_matrix(y_test,predictions))+\"\\n\")  \n",
    "    file.write(\"Classification Report : \\n\" + str(classification_report(y_test,predictions))+\"\\n\")  \n",
    "    file.write(\"Accuracy score : \\n\"+str(accuracy_score(y_test, predictions))+\"\\n\")\n",
    "    file.write(\"Recall Score : \\n\" + str(recall_score(y_test,predictions))+\"\\n\")\n",
    "    file.write(\"F1-score : \\n\" + str(f1_score(y_test, predictions, zero_division=1))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d068eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data :\n",
    "embedding_layer_ft_sg    = Embedding(total_words, embedding_dim, weights=[embedding_matrix_ft_sg], input_length   =max_len_data)\n",
    "embedding_layer_ft_cbow  = Embedding(total_words, embedding_dim, weights=[embedding_matrix_ft_cbow], input_length =max_len_data)\n",
    "embedding_layer_w2v_sg   = Embedding(total_words, embedding_dim, weights=[embedding_matrix_w2v_sg], input_length  =max_len_data)\n",
    "embedding_layer_w2v_cbow = Embedding(total_words, embedding_dim, weights=[embedding_matrix_w2v_cbow], input_length=max_len_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3111c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U numpy==1.18.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61416731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data :\n",
    "model_ft_sg    = model(embedding_layer_ft_sg)\n",
    "model_ft_cbow  = model(embedding_layer_ft_cbow)\n",
    "model_w2v_sg   = model(embedding_layer_w2v_sg)\n",
    "model_w2v_cbow = model(embedding_layer_w2v_cbow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15f48255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,x_test,y_test):\n",
    "    \n",
    "    predictions    = model.predict(x_test)\n",
    "    predictions    = [predictions[i][0] for i in range(len(predictions))]\n",
    "    predict_result = [round(num) for num in predictions]\n",
    "\n",
    "    return predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9a7279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = padding_X_train\n",
    "y_train = y_train\n",
    "\n",
    "X_test = padding_X_test\n",
    "y_test = y_test\n",
    "\n",
    "models = [model_ft_sg,model_ft_cbow,model_w2v_sg,model_w2v_cbow]\n",
    "\n",
    "###############################################################################################################################\n",
    "\n",
    "models_names = [\"FastText_SG\",\"FastText_CBOW\",\"Word2vec_SG\",\"Word2vec_CBOW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5dff1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file to save Results :\n",
    "results = open(\"../CODE/Results/Result_3lstme1.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a0ff523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "030dd6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37398 37398 12466 12466\n",
      "374/374 [==============================] - 650s 2s/step - loss: 0.2746 - accuracy: 0.8900\n",
      "INFO:tensorflow:Assets written to: ../Models/lstm3e1_model_FastText_SG\\assets\n",
      "- finish -------------------------------------------------------------------- Train the LSTM using : FastText_SG\n",
      "37398 37398 12466 12466\n",
      "374/374 [==============================] - 645s 2s/step - loss: 0.5254 - accuracy: 0.7420\n",
      "INFO:tensorflow:Assets written to: ../Models/lstm3e1_model_FastText_CBOW\\assets\n",
      "- finish -------------------------------------------------------------------- Train the LSTM using : FastText_CBOW\n",
      "37398 37398 12466 12466\n",
      "374/374 [==============================] - 665s 2s/step - loss: 0.4189 - accuracy: 0.8133\n",
      "INFO:tensorflow:Assets written to: ../Models/lstm3e1_model_Word2vec_SG\\assets\n",
      "- finish -------------------------------------------------------------------- Train the LSTM using : Word2vec_SG\n",
      "37398 37398 12466 12466\n",
      "374/374 [==============================] - 671s 2s/step - loss: 0.4546 - accuracy: 0.7858\n",
      "INFO:tensorflow:Assets written to: ../Models/lstm3e1_model_Word2vec_CBOW\\assets\n",
      "- finish -------------------------------------------------------------------- Train the LSTM using : Word2vec_CBOW\n"
     ]
    }
   ],
   "source": [
    "for model, name in zip(models, models_names) :\n",
    "    info = \"Train the LSTM using : \"+name\n",
    "    print(len(X_train),len(y_train),len(X_test),len(y_test))\n",
    "    model.fit(X_train, np.array(y_train), epochs = epoch, verbose=1, batch_size= 100)\n",
    "    model.save(\"../Models/lstm3e1_model_\"+name)\n",
    "    evaluate_model(y_test,predict(model,X_test,y_test), results ,info )\n",
    "    print(\"- finish -------------------------------------------------------------------- \"+info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31a3e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close file \n",
    "results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56f5e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "lstm3e1_model_Word2vec_SG = load_model('../Models/lstm3e1_model_Word2vec_SG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "824a59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = [\"ياخي حالة ياخي\"]\n",
    "text = [\"رايع ما\"]\n",
    "#text = [\"مليحةهادي بزاف مليحة  \"]\n",
    "\n",
    "text = pad_sequences(tokenizer.texts_to_sequences(text),maxlen = max_len_data)\n",
    "\n",
    "prediction = lstm3e1_model_Word2vec_SG.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "77a716da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77987254]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a92843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
