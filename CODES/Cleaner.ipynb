{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d78aed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
    "#built on top of the Python programming language.\n",
    "import pandas as pd\n",
    "#NLTK is a leading platform for building Python programs to work with human language data.\n",
    "import nltk\n",
    "\n",
    "#Regular Expression\n",
    "import re\n",
    "\n",
    "#A specific Arabic language library for Python, provides basic functions to manipulate Arabic letters and text, \n",
    "#like detecting Arabic letters, Arabic letters groups and characteristics, remove diacritics etc.\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import tokenize, is_arabicrange, strip_tashkeel, SUN, MOON\n",
    "\n",
    "#This module provides access to the Unicode Character Database (UCD) which defines character properties for all\n",
    "#Unicode characters.\n",
    "import unicodedata\n",
    "#Call nltk.RegexpTokenizer(pattern) with pattern as r\"\\w+\" to create a tokenzier that uses pattern to split a string. \n",
    "#Call RegexpTokenizer.tokenize(text) with RegexpTokenizer as the previous result and text as a string representing a sentence\n",
    "#to return text as a list of words with punctuation's removed.( Remove punctuation and emojis)\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem import ISRIStemmer\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a0175ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    cleaned_data = None\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def tokenize(self, txt):\n",
    "        tokens = word_tokenize(txt)\n",
    "        return tokens\n",
    "\n",
    "    def remove_stop_words(self, tokens):\n",
    "        stops = stopwords.words('arabic')\n",
    "        valuable_words = []\n",
    "        for word in tokens:\n",
    "            if word not in stops:\n",
    "                valuable_words.append(word)\n",
    "        sentence = \" \".join(valuable_words)\n",
    "        return sentence\n",
    "\n",
    "    def remove_stops(self, tokens):\n",
    "        stops = stopwords.words('arabic')\n",
    "        valuable_words = []\n",
    "        for word in tokens:\n",
    "            if word not in stops:\n",
    "                valuable_words.append(word)\n",
    "\n",
    "        return valuable_words\n",
    "    #Laten chars remover\n",
    "    def remove_laten_char(self, text):\n",
    "        text = ''.join((c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn'))\n",
    "        text = re.sub(r'[a-zA-Z]+', '', text)\n",
    "        return text\n",
    "    \n",
    "    #Techkil\n",
    "    def remove_diacritics(self, text):\n",
    "        return araby.strip_tashkeel(text)\n",
    "    \n",
    "    #Normalisation\n",
    "    def clean(self, txt):\n",
    "        clean = re.sub(r'(?is)[-_]', \" \", str(txt))\n",
    "        clean = re.sub(r'(?is)[^أ-ي ❤☻☺]', '', str(clean))\n",
    "        clean = re.sub(\"[إأٱآا]\", \"ا\", clean)\n",
    "        clean = re.sub(\"[إأٱآا]+\", 'ا', clean)\n",
    "        clean = re.sub(\"ى\", \"ي\", clean)\n",
    "        clean = re.sub(\"ؤ\", \"ء\", clean)\n",
    "        clean = re.sub(\"ة\", \"ه\", clean)\n",
    "        clean = re.sub(\"ئ\", \"ء\", clean)\n",
    "        return clean\n",
    "\n",
    "    def remove_url(self, text): \n",
    "        url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        \n",
    "        return url_pattern.sub(r'', text)\n",
    "\n",
    "    def analyze_emotions(self, txt):\n",
    "        emotions = re.sub(\"❤\", \"حب\", txt)\n",
    "        emotions = re.sub(\"☺☻\", \"ضحك\", emotions)\n",
    "        return emotions\n",
    "    \n",
    "    \n",
    "    def prepare_data_set(self, data):\n",
    "\n",
    "        sentence = []\n",
    "\n",
    "        for key, value in data.items():\n",
    "            text = self.clean(key)\n",
    "            text = self.remove_url(text)\n",
    "            text = self.remove_laten_char(text)\n",
    "            text = self.remove_diacritics(text)\n",
    "            text = self.tokenize(text)\n",
    "            text = self.remove_stop_words(text)\n",
    "            text = self.analyze_emotions(text)\n",
    "\n",
    "            sentence.append((text, value))\n",
    "\n",
    "        return sentence\n",
    "\n",
    "    def prepare_data_list(self, data):\n",
    "\n",
    "        sentence = []\n",
    "\n",
    "        for key in data:\n",
    "            text = self.remove_url(key)\n",
    "            text = self.remove_laten_char(text)\n",
    "            text = self.remove_diacritics(text)\n",
    "            text = self.clean(text)\n",
    "            \n",
    "            text = self.tokenize(text)\n",
    "            text = self.remove_stop_words(text)\n",
    "            text = self.analyze_emotions(text)\n",
    "            \n",
    "            sentence.append(text)\n",
    "\n",
    "        return sentence\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e0c573",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1b952dc12974>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../DATA/cleaned_data1.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m#tc.build_pickle(text, label, alg)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "#from DataCleaner import DataCleaner\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as vec\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from TweetClassifier import TweetClassifier\n",
    "\n",
    "dc = DataCleaner()\n",
    "#tc = TweetClassifier()\n",
    "alg = MultinomialNB()\n",
    "dataframe = pd.read_csv(\"../DATA/dataset.csv\", names=[\"tweet\", \"class\"])\n",
    "dataframe.tweet = dc.prepare_data_list(list(dataframe.tweet))\n",
    "dataframe.to_csv(\"../DATA/cleaned_data1.csv\", encoding='utf8')\n",
    "text, label = dataframe.tweet, dataframe[\"class\"]\n",
    "text = tc.feature_extraction(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260312e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
